{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "## - adaptive prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oleg\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from six import iteritems\n",
    "\n",
    "import os\n",
    "from os.path import join as path_join\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmin, savez, asscalar, repeat, prod\n",
    "from numpy import save as save_array\n",
    "from numpy import pi as pi_const\n",
    "\n",
    "from scipy.stats import norm as standard_gaussian\n",
    "\n",
    "from keras.backend import clear_session, shape, random_normal, sqrt\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import Layer, Lambda, Dense, Multiply, Add, Concatenate, Dot, Reshape\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras.backend as kernel\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "sess = tf.Session()\n",
    "set_session(sess)\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating data for experiment\n",
    "\n",
    "sampling_size = 10000\n",
    "test_size = 100\n",
    "\n",
    "data_dim = 3\n",
    "regressor_dim = 2\n",
    "latent_dim = 2\n",
    "\n",
    "latent_mean = 0 * np.ones(latent_dim)\n",
    "latent_cov = 1 * np.eye(latent_dim)\n",
    "latent_vars = np.random.multivariate_normal(mean=latent_mean, cov=latent_cov, size=sampling_size)\n",
    "\n",
    "regressor_mean = 5 * np.ones(regressor_dim)\n",
    "regressor_cov = 2 * np.eye(regressor_dim)\n",
    "regressor_vars = np.random.multivariate_normal(mean=regressor_mean, cov=regressor_cov, size=sampling_size)\n",
    "squared_regressors = np.square(regressor_vars)\n",
    "\n",
    "mults_over_squared_regressor = np.array([[1,0,1],\n",
    "                                         [1,1,0]])\n",
    "mults_over_latent = np.array([[0,1,1],\n",
    "                             [1,0,1]])\n",
    "\n",
    "noise = np.random.multivariate_normal(mean=np.zeros(data_dim), cov=np.eye(data_dim), size=sampling_size)\n",
    "\n",
    "data_y = np.dot(squared_regressors, mults_over_squared_regressor) + np.dot(latent_vars, mults_over_latent) + noise\n",
    "data = {'X': regressor_vars, 'y': data_y}\n",
    "\n",
    "train_X, train_y = data['X'][:sampling_size-test_size], data['y'][:sampling_size-test_size]\n",
    "test_X, test_y = data['X'][sampling_size-test_size:], data['y'][sampling_size-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Variational Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters of the model\n",
    "model_name = 'avb'\n",
    "\n",
    "X_dim = data['X'].shape[1]\n",
    "y_dim = data['y'].shape[1]\n",
    "\n",
    "latent_dim = 2\n",
    "noise_dim = y_dim\n",
    "\n",
    "# training params\n",
    "net_depth = 2\n",
    "net_width = 250\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "\n",
    "schedule = {'iter_discr': 1, 'iter_encdec': 1}\n",
    "optimiser_params = {'encdec': {'lr': 0.001}, 'disc': {'lr': 0.001}}\n",
    "\n",
    "# preparation of output folders\n",
    "temp_dir = os.path.join('output', 'temp')\n",
    "experiment_dir = os.path.join('output', model_name)\n",
    "models_dir = os.path.join(experiment_dir, 'models')\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prior_sampler(inputs, noise_dim, **kwargs):\n",
    "    # standard normal\n",
    "    seed = kwargs.get('seed')\n",
    "    samples_isotropic = random_normal(shape=(shape(inputs)[0], noise_dim), mean=0, stddev=1, seed=seed)\n",
    "    return samples_isotropic\n",
    "\n",
    "\n",
    "def latent_prior_sampler(inputs, latent_dim, **kwargs):\n",
    "    # standard normal\n",
    "    seed = kwargs.get('seed')\n",
    "    samples_isotropic = random_normal(shape=(shape(inputs)[0], latent_dim), mean=0, stddev=1, seed=seed)\n",
    "    return samples_isotropic\n",
    "    \n",
    "    \n",
    "def normal_log_probs(args):\n",
    "    mu, std, x = args \n",
    "    MultiNorm = tf.contrib.distributions.MultivariateNormalDiag(loc=mu, scale_diag=std, name='dec_normal')\n",
    "    log_px = MultiNorm.log_prob(x, name='dec_normal_log_px')\n",
    "    \n",
    "    return log_px\n",
    "\n",
    "\n",
    "class AVBDataIterator(object):\n",
    "    def __init__(self, X_dim, y_dim, latent_dim):\n",
    "        self.X_dim = X_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def iter(self, var1, var2, **kwargs):\n",
    "        mode = kwargs.get('mode')\n",
    "        batch_size = kwargs.get('batch_size')\n",
    "        \n",
    "        var1_size, var2_size = var1.shape[0], var2.shape[0]\n",
    "        if (var1_size != var2_size):\n",
    "            raise AttributeError(\"Data inputs are not same size!\")\n",
    "            \n",
    "        n_batches = var1_size / batch_size\n",
    "        if n_batches - int(n_batches) > 0:\n",
    "            raise AttributeError(\"Data input should be divisible by batch size!\")\n",
    "        \n",
    "        iterator = getattr(self, 'iter_data_{}'.format(mode))\n",
    "        return iterator(var1, var2, int(n_batches), **kwargs), int(n_batches)\n",
    "    \n",
    "    def iter_data_training(self, X, y, n_batches, **kwargs):\n",
    "        shuffle = kwargs.get('shuffle', True)\n",
    "        data_size = X.shape[0]\n",
    "        while True:\n",
    "            indices_new_order = np.arange(data_size)\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indices_new_order)\n",
    "            \n",
    "            for batch_indices in np.split(indices_new_order, n_batches):\n",
    "                yield [X[batch_indices].astype(np.float32), y[batch_indices].astype(np.float32)]\n",
    "\n",
    "    def iter_data_inference(self, X, y, n_batches, **kwargs):\n",
    "        data_size = X.shape[0]\n",
    "        while True:\n",
    "            for batch_indices in np.split(np.arange(data_size), n_batches):\n",
    "                yield [X[batch_indices].astype(np.float32), y[batch_indices].astype(np.float32)]\n",
    "\n",
    "    def iter_data_generation(self, X, latent_samples, n_batches, **kwargs):\n",
    "        data_size = X.shape[0]\n",
    "        while True:\n",
    "            for batch_indices in np.split(np.arange(data_size), n_batches):\n",
    "                yield [X[batch_indices].astype(np.float32), latent_samples[batch_indices].astype(np.float32)]\n",
    "\n",
    "                \n",
    "class FreezableModel(Model):\n",
    "    def __init__(self, inputs, outputs, name='freezable'):\n",
    "        super(FreezableModel, self).__init__(inputs=inputs, outputs=outputs, name=name)\n",
    "        self._trainable_layers = None\n",
    "\n",
    "    def _crawl_trainable_layers(self, freezable_layers_prefix, deep_freeze=True):\n",
    "        if not deep_freeze:\n",
    "            trainable_layers = [layer for layer in self.layers if layer.trainable]\n",
    "            \n",
    "        else:\n",
    "            def recursive_model_crawl(current_layer):\n",
    "                deeper_layers = []\n",
    "                if isinstance(current_layer, Model):\n",
    "                    for l in current_layer.layers:\n",
    "                        if l.trainable:\n",
    "                            deeper_layers += recursive_model_crawl(l)\n",
    "                if current_layer.trainable and (current_layer.name.split('_')[0] in freezable_layers_prefix):\n",
    "                    deeper_layers.append(current_layer)\n",
    "                return deeper_layers\n",
    "            trainable_layers = sum([recursive_model_crawl(layer) for layer in self.layers if layer.trainable], [])\n",
    "\n",
    "        return trainable_layers\n",
    "\n",
    "    def get_trainable_layers(self, freezable_layers_prefix=None, deep_crawl=True):\n",
    "        if self._trainable_layers is None:\n",
    "            self._trainable_layers = self._crawl_trainable_layers(freezable_layers_prefix, deep_crawl)\n",
    "            return self._trainable_layers\n",
    "        else:\n",
    "            return self._trainable_layers\n",
    "\n",
    "    def freeze(self, freezable_layers_prefix=None, deep_freeze=True):\n",
    "        if self._trainable_layers is None:\n",
    "            self._trainable_layers = self._crawl_trainable_layers(freezable_layers_prefix, deep_freeze)\n",
    "            \n",
    "        for layer in self._trainable_layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    def unfreeze(self, unfreezable_layers_prefix=None, deep_unfreeze=True):\n",
    "        if self._trainable_layers is None:\n",
    "            self._trainable_layers = self._crawl_trainable_layers(unfreezable_layers_prefix, deep_unfreeze)\n",
    "            \n",
    "        for layer in self._trainable_layers:\n",
    "            layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StandardEncoder(object):\n",
    "    \"\"\"\n",
    "    An Encoder model is trained to parametrise an arbitrary posterior approximate distribution given some \n",
    "    input x, i.e. q(z|X,y). The model takes as input concatenated data samples and arbitrary noise and produces\n",
    "    a latent encoding:\n",
    "    \n",
    "      X, y                              Input\n",
    "     - - - - - - - - -   \n",
    "       |       Noise: N(0,I)                      \n",
    "       |         |                        \n",
    "       ----------- <-- concatenation    \n",
    "            |                           Encoder model\n",
    "       -----------\n",
    "       | Encoder |                      \n",
    "       -----------\n",
    "            |\n",
    "        Latent space                    Output\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X_dim, y_dim, noise_dim, latent_dim, name='Encoder'):\n",
    "        print(\"Initialising {} with {}-dim data input, {}-dim noise input and {}-dim latent output\".format(name, data_dim, noise_dim, latent_dim))\n",
    "        \n",
    "        self.name = name\n",
    "        self.X_dim = X_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise_dim = noise_dim\n",
    "                \n",
    "        self.input_X = Input(shape=(self.X_dim,), name='enc_data_input_X')\n",
    "        self.input_y = Input(shape=(self.y_dim,), name='enc_data_input_y')\n",
    "        \n",
    "        self.data_prior_sampler = Lambda(data_prior_sampler, name='enc_data_prior_sampler')\n",
    "        self.data_prior_sampler.arguments = {'noise_dim': self.noise_dim, 'seed': seed}\n",
    "        \n",
    "        # internal model\n",
    "        input_X = Input(shape=(self.X_dim,), name='enc_internal_data_input_X')\n",
    "        input_y = Input(shape=(self.y_dim,), name='enc_internal_data_input_y')       \n",
    "        noise_input = Input(shape=(self.noise_dim,), name='enc_internal_noise_input')\n",
    "        input_concat = Concatenate(axis=1, name='enc_internal_input_concat')([input_X, input_y, noise_input])\n",
    "        \n",
    "        encoder_body = Dense(net_width, name='enc_body' + '_0')(input_concat)\n",
    "        encoder_body_a = PReLU()(encoder_body)\n",
    "        for i in range(1, net_depth):\n",
    "            encoder_body = Dense(net_width, name='enc_body' + '_{}'.format(i))(encoder_body_a)\n",
    "            encoder_body_a = PReLU()(encoder_body)\n",
    "        \n",
    "        latent_factors = Dense(self.latent_dim, name='enc_latent')(encoder_body_a)\n",
    "        \n",
    "        encoder_body_model = Model(inputs=[input_X, input_y, noise_input],\n",
    "                                   outputs=latent_factors,\n",
    "                                   name='enc_internal_model')\n",
    "        # ---\n",
    "        \n",
    "        data_input_concat = Concatenate(axis=1, name='enc_data_input')([self.input_X, self.input_y])\n",
    "        sampled_noise = self.data_prior_sampler(data_input_concat)\n",
    "        self.encoder_model = Model(inputs=[self.input_X, self.input_y],\n",
    "                                   outputs=encoder_body_model([self.input_X, self.input_y, sampled_noise]),\n",
    "                                   name='enc_model')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        is_learninig = kwargs.get('is_learning', True)\n",
    "        if is_learninig:\n",
    "            return self.encoder_model(args[0])\n",
    "        else:\n",
    "            return self.encoder_model(args[0])\n",
    "\n",
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "    Discriminator model is adversarially trained against the encoder in order to account \n",
    "    for a D_KL(q(z|X,y) || p(z)) term in the variational loss. The discriminator\n",
    "    architecture takes as input samples from the joint probability distribution of the data `X,y` and a approximate\n",
    "    posterior `z` and from the joint of the data and the prior over `z`:\n",
    "    \n",
    "             -----------\n",
    "       ----> | Encoder |\n",
    "       |     -----------\n",
    "       |         |\n",
    "       |    Approx. posterior --> | \n",
    "       |                          |---> (X,y,z') --|\n",
    "       -------------------------> |                |\n",
    "       |                                           |     -----------------\n",
    "      X,y                                          | --> | Discriminator | --> T(X,y,z) regression output\n",
    "       |                                           |     -----------------\n",
    "       -------------------------> |                |\n",
    "                                  |---> (X,y,z)  --|\n",
    "       Prior p(z): N(0,I) ------> |\n",
    "       \n",
    "    \"\"\"\n",
    "    def __init__(self, X_dim, y_dim, latent_dim, name='Discriminator'):\n",
    "        print(\"Initialising {} with {}-dim data input and {}-dim prior/latent input.\".format(name, X_dim, y_dim, latent_dim))\n",
    "        \n",
    "        self.X_dim = X_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.discriminator_from_prior_model = None\n",
    "        self.discriminator_from_posterior_model = None\n",
    "        \n",
    "        self.input_X = Input(shape=(self.X_dim,), name='disc_data_input_X')\n",
    "        self.input_y = Input(shape=(self.y_dim,), name='disc_data_input_y')\n",
    "        \n",
    "        self.latent_input = Input(shape=(self.latent_dim,), name='disc_latent_input')\n",
    "        \n",
    "        self.latent_prior_sampler = Lambda(latent_prior_sampler, name='disc_latent_prior_sampler')\n",
    "        self.latent_prior_sampler.arguments = {'latent_dim': self.latent_dim, 'seed': seed}\n",
    "        \n",
    "        # internal model\n",
    "        input_X = Input(shape=(self.X_dim,), name='disc_internal_data_input_X')\n",
    "        input_y = Input(shape=(self.y_dim,), name='disc_internal_data_input_y')       \n",
    "        latent_input = Input(shape=(self.latent_dim,), name='disc_internal_latent_input')\n",
    "        \n",
    "        data_input = Concatenate(axis=1, name='disc_internal_data_input')([input_X, input_y])\n",
    "        discriminator_body_data = Dense(net_width, name='disc_body_data' + '_0')(data_input)\n",
    "        discriminator_body_data_a = PReLU()(discriminator_body_data)\n",
    "        for i in range(1, net_depth):\n",
    "            discriminator_body_data = Dense(net_width, name='disc_body_data' + '_{}'.format(i))(discriminator_body_data_a)\n",
    "            discriminator_body_data_a = PReLU()(discriminator_body_data)\n",
    "\n",
    "        discriminator_body_latent = Dense(net_width, name='disc_body_latent' + '_0')(latent_input)\n",
    "        discriminator_body_latent_a = PReLU()(discriminator_body_latent)\n",
    "        for i in range(1, net_depth):\n",
    "            discriminator_body_latent = Dense(net_width, name='disc_body_latent' + '_{}'.format(i))(discriminator_body_latent_a)\n",
    "            discriminator_body_latent_a = PReLU()(discriminator_body_latent)\n",
    "            \n",
    "        discriminator_output = Dot(axes=1, name='disc_output_dot')([discriminator_body_data_a, discriminator_body_latent_a])\n",
    "        discriminator_model = Model(inputs=[input_X, input_y, latent_input], \n",
    "                                    outputs=discriminator_output,\n",
    "                                    name='disc_internal_model')\n",
    "        # ---\n",
    "        \n",
    "        data_input_concat = Concatenate(axis=1, name='disc_data_input')([self.input_X, self.input_y])\n",
    "        prior_distribution = self.latent_prior_sampler(data_input_concat)\n",
    "        from_prior_output = discriminator_model([self.input_X, self.input_y, prior_distribution])\n",
    "        self.discriminator_from_prior_model = Model(inputs=[self.input_X, self.input_y],\n",
    "                                                    outputs=from_prior_output,\n",
    "                                                    name='disc_model_from_prior')\n",
    "        \n",
    "        from_posterior_output = discriminator_model([self.input_X, self.input_y, self.latent_input])\n",
    "        self.discriminator_from_posterior_model = Model(inputs=[self.input_X, self.input_y, self.latent_input],\n",
    "                                                        outputs=from_posterior_output,\n",
    "                                                        name='disc_model_from_posterior')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        from_posterior = kwargs.get('from_posterior', False)\n",
    "        if from_posterior:\n",
    "            return self.discriminator_from_posterior_model(args[0])\n",
    "        else:\n",
    "            return self.discriminator_from_prior_model(args[0])\n",
    "\n",
    "\n",
    "class Decoder(object):\n",
    "    \"\"\"\n",
    "    A Decoder model has inputs comprising of a latent encoding given by an Encoder model, a prior sampler \n",
    "    or other custom input and the raw Encoder data input, which is needed to estimate the reconstructed \n",
    "    data log likelihood. It can be visualised as:\n",
    "     \n",
    "       y      X, Latent\n",
    "       |        |\n",
    "       |    -----------\n",
    "       |    | Decoder |\n",
    "       |    -----------\n",
    "       |        |\n",
    "       |      Output\n",
    "       |    probability    --->  Generated data\n",
    "       |        |\n",
    "       ---> Log Likelihood ---> -(reconstruction loss)\n",
    "    \n",
    "    Note that the reconstruction loss is not used when the model training ends. It serves only the purpose to \n",
    "    define a measure of loss which is optimised. \n",
    "    \"\"\"\n",
    "    def __init__(self, X_dim, y_dim, latent_dim, name='Decoder'):\n",
    "        print(\"Initialising {} with {}-dim latent input and {}-dim data output.\".format(name, latent_dim, data_dim))\n",
    "        \n",
    "        self.name = name\n",
    "        self.X_dim = X_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.input_X = Input(shape=(self.X_dim,), name='dec_data_input_X')\n",
    "        self.input_y = Input(shape=(self.y_dim,), name='dec_data_input_y')     \n",
    "        self.latent_input = Input(shape=(self.latent_dim,), name='dec_latent_input')\n",
    "        \n",
    "        # internal model\n",
    "        input_X = Input(shape=(self.X_dim,), name='dec_internal_data_input_X')\n",
    "        input_y = Input(shape=(self.y_dim,), name='dec_internal_data_input_y')       \n",
    "        latent_input = Input(shape=(self.latent_dim,), name='dec_internal_input')\n",
    "\n",
    "        latent_X_concat = Concatenate(axis=1, name='dec_latent_X_concat')([input_X, latent_input])\n",
    "        generator_body = Dense(net_width, name='dec_body' + '_0')(latent_X_concat)\n",
    "        generator_body_a = PReLU()(generator_body)\n",
    "        for i in range(1, net_depth):\n",
    "            generator_body = Dense(net_width, name='dec_body' + '_{}'.format(i))(generator_body_a)\n",
    "            generator_body_a = PReLU()(generator_body)\n",
    "\n",
    "        mu_params = Dense(self.y_dim, name='dec_mu_params')(generator_body_a)\n",
    "        std_params_raw = Dense(self.y_dim, name='dec_std_params_raw')(generator_body_a)      \n",
    "        std_params = Lambda(lambda x: 1.001 + kernel.elu(x), name='dec_std_params')(std_params_raw)\n",
    "        \n",
    "        log_probs = Lambda(normal_log_probs, name='dec_normal_logprob')([mu_params, std_params, input_y])\n",
    "        \n",
    "        Generator_Model = Model(inputs=[input_X, latent_input], outputs=[mu_params, std_params], name='dec_internal_sampling')\n",
    "        ll_estimator_Model = Model(inputs=[input_X, input_y, latent_input], outputs=log_probs, name='dec_internal_trainable')\n",
    "        # ---\n",
    "        \n",
    "        self.generator = Model(inputs=[self.input_X, self.latent_input],\n",
    "                               outputs=Generator_Model([self.input_X, self.latent_input]),\n",
    "                               name='dec_sampling')\n",
    "        \n",
    "        self.ll_estimator = Model(inputs=[self.input_X, self.input_y, self.latent_input],\n",
    "                                  outputs=ll_estimator_Model([self.input_X, self.input_y, self.latent_input]),\n",
    "                                  name='dec_trainable')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        is_learninig = kwargs.get('is_learning', True)\n",
    "        if is_learninig:\n",
    "            return self.ll_estimator(args[0])\n",
    "        else:\n",
    "            return self.generator(args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AVBDiscriminatorLossLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(AVBDiscriminatorLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def discriminator_loss(discrim_output_prior, discrim_output_posterior, from_logits=False):\n",
    "        if from_logits:\n",
    "            discrim_output_posterior = kernel.sigmoid(discrim_output_posterior)\n",
    "            discrim_output_prior = kernel.sigmoid(discrim_output_prior)\n",
    "            \n",
    "        discriminator_loss = kernel.mean(binary_crossentropy(y_pred=discrim_output_posterior,\n",
    "                                                             y_true=kernel.ones_like(discrim_output_posterior))\n",
    "                                       + binary_crossentropy(y_pred=discrim_output_prior,\n",
    "                                                             y_true=kernel.zeros_like(discrim_output_prior)))\n",
    "        return discriminator_loss\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        discrim_output_prior, discrim_output_posterior = inputs\n",
    "        is_in_logits = kwargs.get('is_in_logits', True)\n",
    "        loss = self.discriminator_loss(discrim_output_prior, discrim_output_posterior, from_logits=is_in_logits)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AVBEncoderDecoderLossLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(AVBEncoderDecoderLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def decoder_loss(data_log_probs, discrim_output_posterior):\n",
    "        norm_factor = 1.0 / prod(kernel.int_shape(data_log_probs)[1:])\n",
    "        kl_divergence = kernel.mean(discrim_output_posterior)\n",
    "        reconstruction_error = -1.0 * kernel.mean(data_log_probs)\n",
    "        return norm_factor * kernel.mean(kl_divergence + reconstruction_error)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        decoder_output_log_probs, discrim_output_posterior = inputs\n",
    "        loss = self.decoder_loss(decoder_output_log_probs, discrim_output_posterior)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdversarialVariationalBayes(object):\n",
    "\n",
    "    def __init__(self, X_dim, y_dim, latent_dim, noise_dim, model_name, optimiser_params):\n",
    "        \n",
    "        self.name = model_name\n",
    "        self.X_dim = X_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        \n",
    "        # define the AVB model\n",
    "        self.encoder = StandardEncoder(X_dim=X_dim, y_dim=y_dim, noise_dim=noise_dim, latent_dim=latent_dim)\n",
    "        self.discriminator = Discriminator(X_dim=X_dim, y_dim=y_dim, latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(X_dim=X_dim, y_dim=y_dim, latent_dim=latent_dim)\n",
    "        \n",
    "        # define model inputs\n",
    "        self.input_X = Input(shape=(self.X_dim,), name='{}_data_input_X'.format(self.name))\n",
    "        self.input_y = Input(shape=(self.y_dim,), name='{}_data_input_y'.format(self.name))\n",
    "        self.latent_input = Input(shape=(self.latent_dim,), name='{}_latent_prior_input'.format(self.name))\n",
    "        \n",
    "        # define intermediate params of the model\n",
    "        posterior_approximation = self.encoder([self.input_X, self.input_y],\n",
    "                                               is_learning=True)\n",
    "        discriminator_output_prior = self.discriminator([self.input_X, self.input_y],\n",
    "                                                        from_posterior=False)\n",
    "        discriminator_output_posterior = self.discriminator([self.input_X, self.input_y, posterior_approximation],\n",
    "                                                            from_posterior=True)\n",
    "        reconstruction_log_likelihood = self.decoder([self.input_X, self.input_y, posterior_approximation],\n",
    "                                                     is_learning=True)\n",
    "\n",
    "        # define loss functions\n",
    "        discriminator_loss = AVBDiscriminatorLossLayer(name='disc_loss')([discriminator_output_prior,\n",
    "                                                                          discriminator_output_posterior],\n",
    "                                                                          is_in_logits=True)\n",
    "        decoder_loss = AVBEncoderDecoderLossLayer(name='dec_loss')([reconstruction_log_likelihood,\n",
    "                                                                    discriminator_output_posterior])\n",
    "        \n",
    "        # define the trainable models\n",
    "        self.avb_trainable_discriminator = FreezableModel(inputs=[self.input_X, self.input_y],\n",
    "                                                          outputs=discriminator_loss,\n",
    "                                                          name='freezable_discriminator')\n",
    "        self.avb_trainable_encoder_decoder = FreezableModel(inputs=[self.input_X, self.input_y],\n",
    "                                                            outputs=decoder_loss,\n",
    "                                                            name='freezable_encoder_decoder')\n",
    "\n",
    "        self.avb_trainable_discriminator.freeze(freezable_layers_prefix=['disc'], deep_freeze=True)\n",
    "        self.avb_trainable_encoder_decoder.unfreeze(unfreezable_layers_prefix=['dec', 'enc'], deep_unfreeze=True)\n",
    "        optimiser_params_encdec = optimiser_params['encdec']\n",
    "        self.avb_trainable_encoder_decoder.compile(optimizer=Adam(**optimiser_params_encdec), loss=None)\n",
    "\n",
    "        self.avb_trainable_discriminator.unfreeze()\n",
    "        self.avb_trainable_encoder_decoder.freeze()\n",
    "        optimiser_params_disc = optimiser_params['disc']\n",
    "        self.avb_trainable_discriminator.compile(optimizer=Adam(**optimiser_params_disc), loss=None)\n",
    "\n",
    "        # define testing models\n",
    "        self.inference_model = Model(inputs=[self.input_X, self.input_y],\n",
    "                                     outputs=self.encoder([self.input_X, self.input_y], is_learning=False))\n",
    "        self.generative_model = Model(inputs=[self.input_X, self.latent_input],\n",
    "                                      outputs=self.decoder([self.input_X, self.latent_input], is_learning=False))\n",
    "        \n",
    "        # make collection of models\n",
    "        self.models_dict = {'avb_trainable_discriminator': None,\n",
    "                            'avb_trainable_encoder_decoder': None}\n",
    "        self.models_dict['avb_trainable_encoder_decoder'] = self.avb_trainable_encoder_decoder\n",
    "        self.models_dict['avb_trainable_discriminator'] = self.avb_trainable_discriminator\n",
    "\n",
    "        # define data iterator\n",
    "        self.data_iterator = AVBDataIterator(X_dim=X_dim, y_dim=y_dim, latent_dim=latent_dim)\n",
    "        \n",
    "    def fit(self, X, y, batch_size, epochs, **kwargs):\n",
    "        discriminator_repetitions = kwargs.get('discriminator_repetitions')\n",
    "\n",
    "        data_iterator, iters_per_epoch = self.data_iterator.iter(X, y, batch_size=batch_size, mode='training', shuffle=True)\n",
    "        history = {'encoderdecoder_loss': [], 'discriminator_loss': []}\n",
    "        epoch_loss = np.inf\n",
    "\n",
    "        for ep in tqdm(range(epochs)):\n",
    "            epoch_loss_history_encdec = []\n",
    "            epoch_loss_history_disc = []\n",
    "            \n",
    "            for it in range(iters_per_epoch):\n",
    "                training_batch = next(data_iterator)\n",
    "                loss_autoencoder = self.avb_trainable_encoder_decoder.train_on_batch(training_batch, None)\n",
    "                epoch_loss_history_encdec.append(loss_autoencoder)\n",
    "                \n",
    "                for _ in range(discriminator_repetitions):\n",
    "                    loss_discriminator = self.avb_trainable_discriminator.train_on_batch(training_batch, None)\n",
    "                    epoch_loss_history_disc.append(loss_discriminator)\n",
    "                    \n",
    "            history['encoderdecoder_loss'].append(epoch_loss_history_encdec)\n",
    "            history['discriminator_loss'].append(epoch_loss_history_disc)\n",
    "            \n",
    "        return history\n",
    "\n",
    "    def infer(self, X, y, **kwargs):\n",
    "        if not hasattr(self, 'data_iterator'):\n",
    "            raise AttributeError(\"Initialise the data iterator in the child classes first!\")\n",
    "        sampling_size = kwargs.get('sampling_size')\n",
    "        batch_size = kwargs.get('batch_size')\n",
    "        \n",
    "        X, y = np.repeat(X, sampling_size, axis=0), np.repeat(y, sampling_size, axis=0)\n",
    "        data_iterator, n_iters = self.data_iterator.iter(X, y, batch_size=batch_size, mode='inference')\n",
    "        latent_samples = self.inference_model.predict_generator(data_iterator, steps=n_iters)\n",
    "        return X, latent_samples\n",
    "\n",
    "    def generate(self, X, **kwargs):\n",
    "        latent_samples = kwargs.get('latent_samples', None)\n",
    "        if latent_samples is None:\n",
    "            n_samples = kwargs.get('n_samples')\n",
    "            # prior sampler\n",
    "            latent_samples = np.random.standard_normal(size=(n_samples, self.latent_dim))\n",
    "        \n",
    "        batch_size = kwargs.get('batch_size')\n",
    "        data_iterator, n_iters = self.data_iterator.iter(X, latent_samples, batch_size=batch_size, mode='generation')\n",
    "        mu, std = self.generative_model.predict_generator(data_iterator, steps=n_iters)\n",
    "        \n",
    "        return_params = kwargs.get('return_params')\n",
    "        if return_params:\n",
    "            return mu, std\n",
    "        else:\n",
    "            # likelihood sampler\n",
    "            if np.min(std) > 0:\n",
    "                print (np.mean(mu, axis=1), np.mean(std, axis=1))\n",
    "                sampled_data = np.random.normal(loc=mu, scale=std)\n",
    "            else:\n",
    "                print (mu, std)\n",
    "            return sampled_data\n",
    "            \n",
    "    def reconstruct(self, X, y, batch_size, **kwargs):\n",
    "        sampling_size = kwargs.get('sampling_size', 1)\n",
    "        X_rep, latent_samples = self.infer(X, y, sampling_size=sampling_size, batch_size=batch_size)\n",
    "        reconstructed_samples = self.generate(X_rep, latent_samples=latent_samples, batch_size=batch_size, return_params=False)\n",
    "        return reconstructed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Encoder with 3-dim data input, 3-dim noise input and 2-dim latent output\n",
      "Initialising Discriminator with 2-dim data input and 3-dim prior/latent input.\n",
      "Initialising Decoder with 2-dim latent input and 3-dim data output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oleg\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:49: UserWarning: Output \"dec_loss\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dec_loss\" during training.\n",
      "C:\\Users\\Oleg\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:54: UserWarning: Output \"disc_loss\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"disc_loss\" during training.\n",
      "100%|██████████████████████████████████████████| 10/10 [00:29<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# initialization of AVB model\n",
    "model = AdversarialVariationalBayes(X_dim=X_dim, y_dim=y_dim, latent_dim=latent_dim, noise_dim=noise_dim,\n",
    "                                    model_name=model_name, optimiser_params=optimiser_params)\n",
    "\n",
    "# training of AVB model\n",
    "training_starttime = datetime.now().isoformat()\n",
    "loss_history = model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs,\n",
    "                         discriminator_repetitions=schedule['iter_discr'])\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.773062   5.2034454 10.893135 ]] [[1.5828562 1.7967187 1.3891052]]\n",
      "[[43.298256 20.942232 22.184954]] [[1.329269   1.4779662  0.68416023]]\n",
      "[[35.662838 12.452222 23.08468 ]] [[1.2868456 1.4458755 0.7948687]]\n",
      "[[63.136806 46.07037  16.709578]] [[1.4535375  1.930746   0.90792745]]\n",
      "[[38.403015   7.0115986 31.40671  ]] [[1.2590497 1.4696846 1.0757662]]\n",
      "[[99.24041  38.764847 62.030304]] [[3.1708844 1.7633792 1.0079141]]\n",
      "[[91.95512  44.41244  48.301132]] [[2.6019354 1.7126775 0.7864224]]\n",
      "[[32.507835 15.250232 16.90889 ]] [[1.1695487  1.4736991  0.77822477]]\n",
      "[[102.11871   60.45593   42.039185]] [[2.7344873 2.0219631 0.9993904]]\n",
      "[[66.99834  25.377342 42.112823]] [[2.0470407 1.4037398 0.7468687]]\n",
      "[[59.912613 31.777058 28.250654]] [[1.7002709  1.5387889  0.68181306]]\n",
      "[[48.037163 26.484219 21.28293 ]] [[1.2998362 1.5420837 0.6567341]]\n",
      "[[66.373024 38.46926  27.964777]] [[1.7809824 1.6495948 0.7161645]]\n",
      "[[31.443792 20.906303 10.155079]] [[1.0467572 1.8149883 0.864958 ]]\n",
      "[[13.846912  8.188354  5.110249]] [[1.7675279 1.9268341 1.2460444]]\n",
      "[[68.36032  32.394325 36.33209 ]] [[1.9423033  1.5287682  0.68212473]]\n",
      "[[68.04863   60.705044   5.7444844]] [[1.2379752 2.416816  1.3516519]]\n",
      "[[64.50637  33.44796  30.999107]] [[1.6371427  1.4904857  0.63699955]]\n",
      "[[51.721806 37.39827  13.973309]] [[1.2080781  1.8870777  0.83775604]]\n",
      "[[55.812313 34.041275 21.417913]] [[1.3526409 1.6403126 0.6852738]]\n",
      "[[36.514637 23.572338 12.048493]] [[0.83431363 1.6504967  0.7777813 ]]\n",
      "[[72.593094 24.527233 48.95145 ]] [[2.336307  1.486607  0.8879263]]\n",
      "[[30.304998 19.098892 10.593821]] [[1.0125337  1.7391479  0.83472574]]\n",
      "[[22.032305  15.116564   5.8518825]] [[1.5287411 2.077394  1.1201321]]\n",
      "[[35.663956 15.712415 19.860834]] [[1.3272247  1.4509246  0.76284826]]\n",
      "[[22.745562   7.9550996 14.456912 ]] [[1.2339418  1.6232547  0.96334046]]\n",
      "[[39.959076  36.93728    1.4320406]] [[0.84724313 2.1103377  1.2083858 ]]\n",
      "[[91.78323  30.40718  62.942173]] [[3.0662754 1.6913185 1.1748091]]\n",
      "[[60.555275 21.231329 39.812267]] [[1.925384  1.4051802 0.766811 ]]\n",
      "[[40.929115 28.272863 12.314122]] [[1.0974158 1.8083724 0.8279444]]\n",
      "[[63.896828 30.601562 33.558075]] [[1.8301098 1.4998603 0.6868403]]\n",
      "[[67.22929  43.89453  23.253517]] [[1.6467835 1.8104023 0.7997786]]\n",
      "[[75.217766 19.88527  56.619263]] [[2.4625208 1.7308943 1.1094404]]\n",
      "[[51.094883 22.664183 28.367893]] [[1.4952949 1.3632506 0.6704953]]\n",
      "[[24.778664 13.10479  11.053763]] [[1.1129146 1.6032159 0.8593756]]\n",
      "[[104.25589   59.42841   45.178833]] [[2.7599866  1.9327533  0.91950536]]\n",
      "[[66.77967  47.276333 19.456018]] [[1.6135694  1.9337144  0.92731506]]\n",
      "[[71.23593  30.717031 41.02688 ]] [[2.092777  1.4955539 0.7127826]]\n",
      "[[58.19868  16.33911  42.437576]] [[1.8447081  1.5289673  0.85822195]]\n",
      "[[56.390274 21.605541 35.01752 ]] [[1.7496235 1.3483483 0.7031773]]\n",
      "[[78.83783  13.502052 67.18711 ]] [[2.8296309 2.1278875 1.8715512]]\n",
      "[[31.670193  8.570712 22.928677]] [[1.1693633 1.4726475 0.8904407]]\n",
      "[[45.868805 20.997551 24.840078]] [[1.4324743 1.4293792 0.6828974]]\n",
      "[[66.786354 36.960777 29.981861]] [[1.8650105 1.6138927 0.7091278]]\n",
      "[[51.034733 24.107695 26.91287 ]] [[1.4936196  1.4311109  0.66212386]]\n",
      "[[56.03889  33.048893 22.840239]] [[1.4481844  1.6623389  0.66817427]]\n",
      "[[25.107891   4.4347095 20.357988 ]] [[1.0227853 1.3929471 1.104795 ]]\n",
      "[[33.995674 16.435186 17.176296]] [[1.1457705 1.5120465 0.7552461]]\n",
      "[[47.08563 28.79275 17.8951 ]] [[1.2018872  1.636105   0.70028585]]\n",
      "[[47.39638  34.423534 12.2072  ]] [[0.9715497 1.8055745 0.8102457]]\n",
      "[[55.940575 31.717426 24.071768]] [[1.4997704 1.5712051 0.6702895]]\n",
      "[[62.623184 21.284662 41.89345 ]] [[1.9771047 1.4268711 0.786318 ]]\n",
      "[[30.878626 14.004064 16.327438]] [[1.0614064  1.5232408  0.78766495]]\n",
      "[[60.141083 24.53193  35.99716 ]] [[1.8699242 1.415201  0.6972306]]\n",
      "[[59.002804 30.122639 28.973217]] [[1.645882   1.5109208  0.65676385]]\n",
      "[[45.264805 26.017921 18.983957]] [[1.2695069 1.5901924 0.6900755]]\n",
      "[[83.25366  58.339165 24.79629 ]] [[2.0004334 2.0343847 1.0419936]]\n",
      "[[28.44135  11.924361 16.0918  ]] [[1.1109536 1.557232  0.8186439]]\n",
      "[[44.525814 22.588129 21.684242]] [[1.2915324 1.4753689 0.6761469]]\n",
      "[[77.925995 42.290173 35.933304]] [[2.110803  1.670063  0.7147073]]\n",
      "[[23.349981  8.10762  14.615192]] [[1.1961657 1.6202955 0.9975576]]\n",
      "[[54.34153  26.807184 27.494427]] [[1.5185117  1.452087   0.65530825]]\n",
      "[[43.31831  20.497826 22.602795]] [[1.318841   1.4329517  0.68802947]]\n",
      "[[41.89324  10.045775 31.841272]] [[1.2867789 1.459628  0.876709 ]]\n",
      "[[97.95277  41.406578 57.838764]] [[3.03595    1.7651258  0.99031633]]\n",
      "[[45.426956  42.089344   1.6722484]] [[0.8238201 2.1571586 1.2441771]]\n",
      "[[73.1376   43.480186 29.773205]] [[1.8946292  1.7360476  0.73306197]]\n",
      "[[51.18585  31.295815 19.474253]] [[1.2265925 1.6449515 0.6723679]]\n",
      "[[53.443424 21.37968  32.355404]] [[1.7161287 1.3735788 0.6925361]]\n",
      "[[117.97973   74.81172   43.781612]] [[3.456024  2.5473177 1.4975231]]\n",
      "[[43.287857 14.282243 29.025051]] [[1.4053038 1.4131666 0.7488547]]\n",
      "[[25.526989  3.215208 22.01561 ]] [[1.0182374 1.3828021 1.2375059]]\n",
      "[[56.90215  28.782663 28.194038]] [[1.6221799 1.4878821 0.6763554]]\n",
      "[[65.56796  21.529669 44.741108]] [[2.1034136 1.4662474 0.8498506]]\n",
      "[[23.426313  9.847662 13.189501]] [[1.2437344 1.6173606 0.902055 ]]\n",
      "[[37.181526 19.861347 16.901928]] [[1.1245352  1.5588515  0.73187643]]\n",
      "[[47.47458  23.66287  23.423494]] [[1.2494459  1.4417249  0.64668244]]\n",
      "[[80.44067  46.59117  34.127396]] [[2.1724007  1.7752109  0.79319215]]\n",
      "[[60.228516 24.720896 35.90338 ]] [[1.8861072 1.422348  0.7093927]]\n",
      "[[41.89587  14.966213 26.837612]] [[1.3569249 1.4129852 0.7366217]]\n",
      "[[45.315636 35.174397  9.03146 ]] [[0.9092454 1.8949802 0.8395002]]\n",
      "[[87.349365 36.267338 52.217495]] [[2.7502475 1.6603959 0.8774531]]\n",
      "[[39.725136 27.632654 11.393336]] [[0.9324948  1.728867   0.81797415]]\n",
      "[[70.31373  57.78657  11.402031]] [[1.6231254 2.3273025 1.1912084]]\n",
      "[[111.49996   42.155956  71.28557 ]] [[3.6678545 1.931162  1.2827353]]\n",
      "[[61.0633   36.55864  24.494734]] [[1.6174443  1.6945348  0.70521545]]\n",
      "[[38.684284 22.996643 15.229929]] [[1.0852265  1.6530286  0.74358505]]\n",
      "[[80.82085  41.018444 40.29127 ]] [[2.230671  1.6534842 0.7049381]]\n",
      "[[59.87411  32.716606 27.024746]] [[1.5655326 1.5220587 0.6525789]]\n",
      "[[37.82377  14.793884 23.053738]] [[1.3841649  1.4428271  0.76015717]]\n",
      "[[48.015926 42.48717   4.762908]] [[1.3358301 2.2794292 1.255615 ]]\n",
      "[[61.069992 21.716307 39.793976]] [[1.8995345 1.3942691 0.740422 ]]\n",
      "[[72.147255 46.451122 25.564934]] [[1.7158742 1.8101627 0.7758001]]\n",
      "[[74.73834  43.4661   31.400963]] [[1.9436606  1.7125642  0.72493094]]\n",
      "[[101.96265   54.810547  47.63553 ]] [[2.7795556 1.8452864 0.8678828]]\n",
      "[[51.053234 30.42456  20.285542]] [[1.2676947  1.6315024  0.66151214]]\n",
      "[[40.90835  32.185394  7.535091]] [[0.86789453 1.9351398  0.8524674 ]]\n",
      "[[82.9291  28.17253 55.98479]] [[2.7369497 1.5795696 1.0377967]]\n",
      "[[48.992847 24.801088 24.088646]] [[1.4277049  1.4679767  0.67147845]]\n",
      "[[38.124203 24.897255 12.654464]] [[0.9816501  1.7037456  0.78938043]]\n",
      "rmse of reconstruction:  [0.06248723 0.09351149 0.04899481]\n"
     ]
    }
   ],
   "source": [
    "# Reconstruction test\n",
    "y_preds = []\n",
    "\n",
    "for X_true_i, y_true_i in zip(test_X, test_y):\n",
    "    x_t_i = np.array([X_true_i])\n",
    "    y_t_i = np.array([y_true_i])\n",
    "    \n",
    "    y_recs_i = model.reconstruct(x_t_i, y_t_i, sampling_size=25, batch_size=25)\n",
    "    y_pred_i = np.mean(y_recs_i[0], axis=0)\n",
    "    y_preds.append(y_pred_i)\n",
    "    \n",
    "    \"\"\"\n",
    "    print (y_true_i)\n",
    "    print (y_pred_i)\n",
    "    print ()\n",
    "    \"\"\"\n",
    "    \n",
    "y_preds = np.array(y_preds)\n",
    "rmse = np.std(y_preds - test_y, axis = 0)\n",
    "std = np.std(test_y, axis=0)\n",
    "\n",
    "print('rmse of reconstruction: ', rmse/std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
